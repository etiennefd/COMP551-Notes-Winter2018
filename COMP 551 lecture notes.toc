\contentsline {section}{\numberline {1}Introduction to machine learning}{6}
\contentsline {subsection}{\numberline {1.1}What can machines do?}{6}
\contentsline {subsection}{\numberline {1.2}What is machine learning?}{6}
\contentsline {subsection}{\numberline {1.3}Prediction problems}{6}
\contentsline {subsection}{\numberline {1.4}What is learning?}{7}
\contentsline {section}{\numberline {2}Introduction to machine learning, continued}{8}
\contentsline {subsection}{\numberline {2.1}Curve fitting example}{8}
\contentsline {subsubsection}{\numberline {2.1.1}Test set}{9}
\contentsline {subsubsection}{\numberline {2.1.2}Model selection}{9}
\contentsline {subsection}{\numberline {2.2}Machine learning pipeline}{10}
\contentsline {subsection}{\numberline {2.3}Classification example}{10}
\contentsline {subsubsection}{\numberline {2.3.1}Model 1: linear model}{11}
\contentsline {subsubsection}{\numberline {2.3.2}Model 2: nearest-neighbor}{12}
\contentsline {subsubsection}{\numberline {2.3.3}Least squares vs. nearest-neighbors}{13}
\contentsline {section}{\numberline {3}Statistical decision theory}{14}
\contentsline {subsection}{\numberline {3.1}Expected prediction error}{14}
\contentsline {subsection}{\numberline {3.2}Expected prediction error for $k$-nearest neighbors}{15}
\contentsline {subsection}{\numberline {3.3}Local methods in high dimensions}{15}
\contentsline {subsection}{\numberline {3.4}Expected prediction error for linear regression}{16}
\contentsline {subsection}{\numberline {3.5}Inductive bias}{16}
\contentsline {subsection}{\numberline {3.6}Non-squared error loss}{16}
\contentsline {subsection}{\numberline {3.7}Expected prediction error for classification}{17}
\contentsline {section}{\numberline {4}Linear regression}{18}
\contentsline {subsection}{\numberline {4.1}Linear basis function models}{18}
\contentsline {subsubsection}{\numberline {4.1.1}Polynomial basis function}{18}
\contentsline {subsubsection}{\numberline {4.1.2}Gaussian basis function}{18}
\contentsline {subsubsection}{\numberline {4.1.3}Sigmoidal basis functions}{19}
\contentsline {subsubsection}{\numberline {4.1.4}Identity basis function}{19}
\contentsline {subsection}{\numberline {4.2}Least squares}{19}
\contentsline {subsubsection}{\numberline {4.2.1}Geometry of least squares}{19}
\contentsline {subsection}{\numberline {4.3}Learning by gradient descent}{20}
\contentsline {subsubsection}{\numberline {4.3.1}Gradient descent outline}{20}
\contentsline {subsubsection}{\numberline {4.3.2}Linear regression example}{21}
\contentsline {subsection}{\numberline {4.4}The special nature of the linear regression loss function}{21}
\contentsline {subsection}{\numberline {4.5}Online or sequential learning}{21}
\contentsline {subsubsection}{\numberline {4.5.1}Online gradient descent algorithm}{22}
\contentsline {subsection}{\numberline {4.6}Multiple outputs}{22}
\contentsline {section}{\numberline {5}Bias-variance tradeoff, regularization}{23}
\contentsline {subsection}{\numberline {5.1}Empirical risk minimization}{23}
\contentsline {subsection}{\numberline {5.2}Bias-variance decomposition}{23}
\contentsline {subsubsection}{\numberline {5.2.1}Occam's razor}{25}
\contentsline {subsubsection}{\numberline {5.2.2}Regression example}{26}
\contentsline {subsection}{\numberline {5.3}Regularization}{26}
\contentsline {subsubsection}{\numberline {5.3.1}L2 regularization}{26}
\contentsline {subsubsection}{\numberline {5.3.2}L1 regularization}{27}
\contentsline {section}{\numberline {6}Linear classification}{28}
\contentsline {subsection}{\numberline {6.1}Approaches to classification}{28}
\contentsline {subsubsection}{\numberline {6.1.1}Generative models}{28}
\contentsline {subsubsection}{\numberline {6.1.2}Discriminative models}{29}
\contentsline {subsubsection}{\numberline {6.1.3}Discriminant-based models}{29}
\contentsline {subsection}{\numberline {6.2}Linearly separable problems}{29}
\contentsline {subsection}{\numberline {6.3}Target variable for classification}{29}
\contentsline {subsection}{\numberline {6.4}Discriminant functions}{30}
\contentsline {subsubsection}{\numberline {6.4.1}Two-class scenario}{30}
\contentsline {subsubsection}{\numberline {6.4.2}Multiple classes scenario}{31}
\contentsline {section}{\numberline {7}Indicator regression, PCA, LDA}{33}
\contentsline {subsection}{\numberline {7.1}Least squares for classification}{33}
\contentsline {subsection}{\numberline {7.2}Principal component analysis (PCA)}{34}
\contentsline {subsubsection}{\numberline {7.2.1}Formal definition}{34}
\contentsline {subsubsection}{\numberline {7.2.2}Procedure}{35}
\contentsline {subsubsection}{\numberline {7.2.3}Can we use PCA dimensions for classification?}{35}
\contentsline {subsection}{\numberline {7.3}Linear discriminant analysis (LDA)}{35}
\contentsline {subsubsection}{\numberline {7.3.1}Solution proposed by Fisher}{36}
\contentsline {section}{\numberline {8}GLMs, GDA, evaluation metrics}{38}
\contentsline {subsection}{\numberline {8.1}Generalized linear models}{38}
\contentsline {subsection}{\numberline {8.2}Probabilistic generative models}{38}
\contentsline {subsubsection}{\numberline {8.2.1}2-class problem}{39}
\contentsline {subsubsection}{\numberline {8.2.2}$k$-class problem, $k>2$}{39}
\contentsline {subsection}{\numberline {8.3}Gaussian discriminant analysis (GDA)}{40}
\contentsline {subsubsection}{\numberline {8.3.1}Two-class case}{40}
\contentsline {subsubsection}{\numberline {8.3.2}Learning the parameters}{41}
\contentsline {subsubsection}{\numberline {8.3.3}Maximum likelohood solution}{42}
\contentsline {subsection}{\numberline {8.4}Evaluation metrics for classification}{43}
\contentsline {section}{\numberline {9}Na\IeC {\"\i }ve Bayes, logistic regression}{46}
\contentsline {subsection}{\numberline {9.1}Gaussian Na\IeC {\"\i }ve Bayes}{46}
\contentsline {subsection}{\numberline {9.2}Summary of methods for continuous input features}{46}
\contentsline {subsection}{\numberline {9.3}Na\IeC {\"\i }ve Bayes with discrete features}{47}
\contentsline {subsubsection}{\numberline {9.3.1}Two-class problem}{47}
\contentsline {subsubsection}{\numberline {9.3.2}Example application: Text classification}{48}
\contentsline {subsubsection}{\numberline {9.3.3}Laplace smoothing}{49}
\contentsline {subsection}{\numberline {9.4}Probabilistic discriminative models}{49}
\contentsline {subsection}{\numberline {9.5}Logistic regression model}{50}
\contentsline {subsubsection}{\numberline {9.5.1}Maximum likelihood for logistic regression}{50}
\contentsline {subsection}{\numberline {9.6}Maximum likelihood and least squares}{51}
\contentsline {section}{\numberline {10}Newton-Raphson method, Perceptron}{53}
\contentsline {subsection}{\numberline {10.1}Iterative reweighted least squares}{53}
\contentsline {subsubsection}{\numberline {10.1.1}Newton-Raphson method}{53}
\contentsline {subsubsection}{\numberline {10.1.2}Example: linear regression}{54}
\contentsline {subsubsection}{\numberline {10.1.3}Geometric view}{54}
\contentsline {subsubsection}{\numberline {10.1.4}Newton-Raphson for logistic regression}{54}
\contentsline {subsection}{\numberline {10.2}Pros and cons of generative, discriminative, and discriminant-based models for classification}{55}
\contentsline {subsubsection}{\numberline {10.2.1}Generative models}{55}
\contentsline {subsubsection}{\numberline {10.2.2}Generative vs. discriminative}{56}
\contentsline {subsubsection}{\numberline {10.2.3}Discriminant-based vs. discriminative}{56}
\contentsline {subsubsection}{\numberline {10.2.4}Advantages of $P(C_k|x)$ (discriminative models)}{56}
\contentsline {subsection}{\numberline {10.3}Summary of the course so far}{57}
\contentsline {subsection}{\numberline {10.4}Separating hyperplane methods}{57}
\contentsline {subsection}{\numberline {10.5}Perceptron algorithm}{58}
\contentsline {subsubsection}{\numberline {10.5.1}Perceptron error criterion}{58}
\contentsline {subsubsection}{\numberline {10.5.2}Algorithm pseudocode}{59}
\contentsline {subsubsection}{\numberline {10.5.3}Perceptron convergence theorem}{59}
\contentsline {subsubsection}{\numberline {10.5.4}Issues}{59}
\contentsline {section}{\numberline {11}Separating hyperplanes, SVM}{60}
\contentsline {subsection}{\numberline {11.1}Max-margin classifier}{60}
\contentsline {subsection}{\numberline {11.2}Scaling}{60}
\contentsline {subsection}{\numberline {11.3}Canonical representation of decision boundaries}{60}
\contentsline {subsubsection}{\numberline {11.3.1}Lagrangian formulation of the problem}{61}
\contentsline {subsubsection}{\numberline {11.3.2}Finding $w, b$ after solving the dual problem}{62}
\contentsline {subsubsection}{\numberline {11.3.3}Error function}{63}
\contentsline {subsection}{\numberline {11.4}Overlapping class distributions}{63}
\contentsline {section}{\numberline {12}SVM, Non-parametric Methods, Decision Trees}{65}
\contentsline {subsection}{\numberline {12.1}Wrapping up Support Vector Machines (SVM)}{65}
\contentsline {subsection}{\numberline {12.2}Parametric vs. non-parametric methods}{65}
\contentsline {subsection}{\numberline {12.3}Power of basis functions}{66}
\contentsline {subsection}{\numberline {12.4}Decision trees}{66}
\contentsline {subsubsection}{\numberline {12.4.1}Hunt's algorithm}{66}
\contentsline {subsection}{\numberline {12.5}How to specify the attribute test condition?}{67}
\contentsline {subsection}{\numberline {12.6}How to determine the best split?}{67}
\contentsline {subsubsection}{\numberline {12.6.1}Gini index}{67}
\contentsline {subsubsection}{\numberline {12.6.2}Entropy}{68}
\contentsline {subsubsection}{\numberline {12.6.3}Classification error}{68}
\contentsline {subsection}{\numberline {12.7}When to stop splitting?}{68}
\contentsline {subsubsection}{\numberline {12.7.1}Reduced error pruning}{69}
\contentsline {subsubsection}{\numberline {12.7.2}Rule post-pruning}{69}
\contentsline {subsection}{\numberline {12.8}Advantages of decision trees}{69}
\contentsline {section}{\numberline {13}Ensemble Learning: Bagging, Boosting}{70}
\contentsline {subsection}{\numberline {13.1}Decision trees as collections of classifiers}{70}
\contentsline {subsection}{\numberline {13.2}Ensemble models}{70}
\contentsline {subsubsection}{\numberline {13.2.1}Average}{70}
\contentsline {subsubsection}{\numberline {13.2.2}Bootstrap datasets}{70}
\contentsline {subsection}{\numberline {13.3}Bootstrap aggregation (bagging)}{71}
\contentsline {subsection}{\numberline {13.4}Random forests}{72}
\contentsline {subsection}{\numberline {13.5}Extremely randomized trees}{72}
\contentsline {subsection}{\numberline {13.6}Boosting}{73}
\contentsline {subsubsection}{\numberline {13.6.1}Adaptive boosting (AdaBoost) algorithm}{73}
\contentsline {subsubsection}{\numberline {13.6.2}Example}{74}
\contentsline {subsection}{\numberline {13.7}Minimizing exponential error}{74}
\contentsline {section}{\numberline {14}Stacking, Neural Networks}{76}
\contentsline {subsection}{\numberline {14.1}Exponential error function}{76}
\contentsline {subsection}{\numberline {14.2}Bagging vs. boosting}{76}
\contentsline {subsection}{\numberline {14.3}Side-note: Cross-validation}{77}
\contentsline {subsection}{\numberline {14.4}Stacked generalization}{77}
\contentsline {subsubsection}{\numberline {14.4.1}Algorithm}{77}
\contentsline {subsection}{\numberline {14.5}Neural networks}{77}
\contentsline {subsubsection}{\numberline {14.5.1}The infamous XOR problem}{77}
\contentsline {subsubsection}{\numberline {14.5.2}Representation learning}{78}
\contentsline {subsubsection}{\numberline {14.5.3}Example: prediction problem}{78}
\contentsline {subsubsection}{\numberline {14.5.4}Some standard activation functions $g$}{78}
\contentsline {subsubsection}{\numberline {14.5.5}Some output activation functions $o$}{78}
\contentsline {subsubsection}{\numberline {14.5.6}Universal approximation theorem}{79}
\contentsline {subsubsection}{\numberline {14.5.7}More layers}{79}
\contentsline {section}{\numberline {15}Neural Networks and Backpropagation}{80}
\contentsline {subsection}{\numberline {15.1}Learning in neural networks}{80}
\contentsline {subsection}{\numberline {15.2}Backpropagation overview}{80}
\contentsline {subsection}{\numberline {15.3}Derivatives of network functions}{80}
\contentsline {subsubsection}{\numberline {15.3.1}B-diagram (backpropagation diagram)}{80}
\contentsline {subsubsection}{\numberline {15.3.2}Feed-forward stage}{81}
\contentsline {subsubsection}{\numberline {15.3.3}Backpropagation stage}{81}
\contentsline {subsection}{\numberline {15.4}Backpropagation algorithm}{81}
\contentsline {subsubsection}{\numberline {15.4.1}Proof}{82}
\contentsline {subsection}{\numberline {15.5}Learning with backpropagation}{82}
\contentsline {subsection}{\numberline {15.6}The case of layered networks}{82}
\contentsline {subsubsection}{\numberline {15.6.1}Steps of the algorithm}{82}
\contentsline {subsection}{\numberline {15.7}More than one training examples}{82}
\contentsline {subsection}{\numberline {15.8}Backpropagation in matrix form}{83}
\contentsline {section}{\numberline {16}Training deep neural networks}{84}
\contentsline {subsection}{\numberline {16.1}Deep neural networks}{84}
\contentsline {subsection}{\numberline {16.2}Solution 1: Greedy layerwise pretraining}{84}
\contentsline {subsection}{\numberline {16.3}Solution 2: Rectified linear units (ReLU)}{85}
\contentsline {subsubsection}{\numberline {16.3.1}Solution: Leaky ReLU}{85}
\contentsline {subsection}{\numberline {16.4}Solution 3: Batch normalization}{86}
