\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to machine learning}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}What can machines do?}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}What is machine learning?}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Prediction problems}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}What is learning?}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction to machine learning, continued}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Curve fitting example}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Test set}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Model selection}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine learning pipeline}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Classification example}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Model 1: linear model}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Model 2: nearest-neighbor}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Least squares vs. nearest-neighbors}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Statistical decision theory}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Expected prediction error}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Expected prediction error for $k$-nearest neighbors}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Local methods in high dimensions}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Expected prediction error for linear regression}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Inductive bias}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Non-squared error loss}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Expected prediction error for classification}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Linear regression}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Linear basis function models}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Polynomial basis function}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Gaussian basis function}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Sigmoidal basis functions}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Identity basis function}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Least squares}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Geometry of least squares}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Learning by gradient descent}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Gradient descent outline}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Linear regression example}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}The special nature of the linear regression loss function}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Online or sequential learning}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Online gradient descent algorithm}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Multiple outputs}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Bias-variance tradeoff, regularization}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Empirical risk minimization}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Bias-variance decomposition}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Occam's razor}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Regression example}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Regularization}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}L2 regularization}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}L1 regularization}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Linear classification}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Approaches to classification}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Generative models}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Discriminative models}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Discriminant-based models}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Linearly separable problems}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Target variable for classification}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Discriminant functions}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Two-class scenario}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Multiple classes scenario}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Indicator regression, PCA, LDA}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Least squares for classification}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Principal component analysis (PCA)}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Formal definition}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Procedure}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Can we use PCA dimensions for classification?}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Linear discriminant analysis (LDA)}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Solution proposed by Fisher}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {8}GLMs, GDA, evaluation metrics}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Generalized linear models}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Probabilistic generative models}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}2-class problem}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}$k$-class problem, $k>2$}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Gaussian discriminant analysis (GDA)}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}Two-class case}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}Learning the parameters}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.3}Maximum likelohood solution}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Evaluation metrics for classification}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Na\IeC {\"\i }ve Bayes, logistic regression}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Gaussian Na\IeC {\"\i }ve Bayes}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Summary of methods for continuous input features}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Na\IeC {\"\i }ve Bayes with discrete features}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Two-class problem}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Example application: Text classification}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.3}Laplace smoothing}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Probabilistic discriminative models}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Logistic regression model}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}Maximum likelihood for logistic regression}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Maximum likelihood and least squares}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Newton-Raphson method, Perceptron}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Iterative reweighted least squares}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}Newton-Raphson method}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}Example: linear regression}{54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.3}Geometric view}{54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.4}Newton-Raphson for logistic regression}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Pros and cons of generative, discriminative, and discriminant-based models for classification}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}Generative models}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Generative vs. discriminative}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.3}Discriminant-based vs. discriminative}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.4}Advantages of $P(C_k|x)$ (discriminative models)}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Summary of the course so far}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Separating hyperplane methods}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Perceptron algorithm}{58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.1}Perceptron error criterion}{58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.2}Algorithm pseudocode}{59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.3}Perceptron convergence theorem}{59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4}Issues}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Separating hyperplanes, SVM}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Max-margin classifier}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Scaling}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Canonical representation of decision boundaries}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1}Lagrangian formulation of the problem}{61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2}Finding $w, b$ after solving the dual problem}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.3}Error function}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Overlapping class distributions}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {12}SVM, Non-parametric Methods, Decision Trees}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Wrapping up Support Vector Machines (SVM)}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Parametric vs. non-parametric methods}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Power of basis functions}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}Decision trees}{66}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1}Hunt's algorithm}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}How to specify the attribute test condition?}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}How to determine the best split?}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.1}Gini index}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.2}Entropy}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.3}Classification error}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}When to stop splitting?}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.7.1}Reduced error pruning}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.7.2}Rule post-pruning}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8}Advantages of decision trees}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Ensemble Learning: Bagging, Boosting}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Decision trees as collections of classifiers}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Ensemble models}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Average}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.2}Bootstrap datasets}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Bootstrap aggregation (bagging)}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Random forests}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5}Extremely randomized trees}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6}Boosting}{73}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.6.1}Adaptive boosting (AdaBoost) algorithm}{73}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.6.2}Example}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.7}Minimizing exponential error}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Stacking, Neural Networks}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Exponential error function}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Bagging vs. boosting}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Side-note: Cross-validation}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Stacked generalization}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.4.1}Algorithm}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5}Neural networks}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.1}The infamous XOR problem}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.2}Representation learning}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.3}Example: prediction problem}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.4}Some standard activation functions $g$}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.5}Some output activation functions $o$}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.6}Universal approximation theorem}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.7}More layers}{79}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Neural Networks and Backpropagation}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Learning in neural networks}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Backpropagation overview}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Derivatives of network functions}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.3.1}B-diagram (backpropagation diagram)}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.3.2}Feed-forward stage}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.3.3}Backpropagation stage}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Backpropagation algorithm}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.1}Proof}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Learning with backpropagation}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}The case of layered networks}{82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.6.1}Steps of the algorithm}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7}More than one training examples}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.8}Backpropagation in matrix form}{83}}
\@writefile{toc}{\contentsline {section}{\numberline {16}Training deep neural networks}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Deep neural networks}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2}Solution 1: Greedy layerwise pretraining}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3}Solution 2: Rectified linear units (ReLU)}{85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.3.1}Solution: Leaky ReLU}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4}Solution 3: Batch normalization}{86}}
